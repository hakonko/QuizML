_question;_latex;_alt1;_alt2;_alt3;_alt4;_alt5;_correct_alt;_genre;_pid
In logistic regression for binary classification, what is the meaning/interpretation of the output of the sigmoid function for an input $x$?;;Estimate of parameters w;Cross-entropy loss value;Predicted probability $Pr(\{y=1 | x\})$;True label;Bias;_alt3;Linear Models;1
Which of the following options explains why convolutional neural networks became popular?;;Parameter sharing;Localized Pattern Detection;Hierarchical Pattern Recognition across Layers;All options;Pooling layers;_alt4;Neural Nets;2
What is not an advantage of residual connection in deep neural networks?;;Identity+ optional non-linearity;Never worse than identity;If a convolution block is not useful, it will be bypassed;Visual info processed at various scales & concatenated along depth;Helps reduce the number of parameters in the network;_alt4;CNNs;3
"A classification model is evaluated on 3 classes: A, B, and C. For each class, you are given the precision and recall values at each relevant threshold.The average precision (AP) is defined as the area under the precision-recall curve for each class.
Suppose we approximate this area using the 11-point interpolated Average Precision method, where precision is measured at 11 recall levels: $\{0.0, 0.1, 0.2, ..., 1.0\}$. For each recall level $r$, the precision is defined as the maximum precision obtained for any recall $\geq r$. You are given the following interpolated precision values:
Class A: $[1.0, 1.0, 1.0, 0.9, 0.9, 0.85, 0.8, 0.7, 0.6, 0.5, 0.5]$
Class B: $[1.0, 1.0, 0.9, 0.9, 0.85, 0.8, 0.75, 0.7, 0.6, 0.6, 0.5]$
Class C: $[0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.6, 0.55, 0.5, 0.4]$";mAP = \frac{1}{N} \sum_{i=1}^N AP_i,  \text{ where N is the number of classes};0.700;0.727;0.750;0.800;0.833;_alt3;"Performance Estimation
";4
A 1D convolutional layer has kernel size 3 and stride 2. A second layer on top uses kernel size 5 and stride 3. A third layer uses kernel size 3 and stride 1. How many input elements from the original input affect one output value of the third layer (assume no padding, center neuron)?;;15;11;17;21;13;_alt3;CNNs;5
Which of the following is true about dropout?;;Dropout removes neurons during inference.;Dropout reduces training time significantly.;Dropout prevents co-adaptation and improves generalization.;Dropout increases the number of trainable parameters.;Dropout enforces weight sharing.;_alt3;Neural Nets;6
Given a $3 \times 2$ input matrix and a transposed convolution with kernel size $2 \times 2$, stride 2, and no padding, what is the spatial size of the output?;\text{Output} = \text{Stride} \times (\text{Input size} - 1) + \text{Kernel size};$6 \times 4$;$5 \times 3$;$7 \times 5$;$4 \times 2$;$3 \times 2$;_alt1;CNNs;7
In a binary classification problem, we care more about detecting rare positive cases (label B). Which metric is most suitable?;;Accuracy;Specificity;Precision;Recall (Sensitivity);F1-score;_alt4;Performance Estimation;8
Given empirical risk minimization: How do you adjust it for importance weighting with known density ratios $r(x,y)$?;\min_{\theta} \frac{1}{N} \sum_{n=1}^N \ell(h_\theta(x_n), y_n);Multiply the loss by the predicted label;Add a penalty term based on class imbalance;Replace the loss with KL divergence;Replace $l$ with $r(x_n, y_n)\cdot l$;Replace N with test set size;_alt4;Performance Estimation;9
A 2D convolution has input shape $(112, 224)$, kernel size $(5, 5)$, stride 4, and padding 5. What is the spatial output shape?;\text{Output size} = \left\lfloor \frac{N + 2P - K}{S} \right\rfloor + 1;$(29, 57)$;$(28, 56)$;$(27, 55)$;$(26, 54)$;$(30, 58)$;_alt1;CNNs;10
Why is stochastic gradient descent (SGD) often preferred over plain gradient descent (GD) for training deep networks?;;SGD always finds the global minimum;SGD uses less memory by removing the optimizer;SGD avoids vanishing gradients;SGD provides noisy gradients that help escape poor local minima;SGD automatically adjusts the learning rate;_alt4;Neural Nets;11
In object detection, how do we assign predicted boxes to ground truth boxes during training?;;Based on pixel-wise accuracy;Using histogram equalization;Using Intersection over Union (IoU) threshold;By minimizing reconstruction loss;By matching identical coordinates;_alt3;Backprop and Optimization;12
Which combination best describes a targeted white-box attack?;;Attacker doesn’t know the model but can choose target class;Attacker knows model and tries to cause any misclassification;Attacker knows model and tries to classify as a specific wrong clas;Attacker has no access to gradients and perturbs randomly;Attacker uses PGD in a black-box setting;_alt3;Adversarial Attacks;13
You are building a segmentation model that must both segment the tumor region and assign a grade (1–4) per pixel. How many output channels should the final layer have?;;1;2;4;5;6;_alt4;Image Segmentation;14
You want to train a model to both classify and localize objects using bounding boxes. Which combination of losses is appropriate?;;Binary cross-entropy + Dice loss;Cross-entropy + L2 loss;Focal loss only;L1 loss only;IoU loss only;_alt2;Object Detection;15
Which of the following is a possible loss function for the generator in a GAN?;\min_G \log(1 - D(G(z)));Encourages discriminator to output 1;Encourages discriminator to output 0;Encourages generator to fool the discriminator;Maximizes likelihood of real data;Minimizes KL divergence;_alt3;Deep Architectures;16
What is the purpose of adding momentum to SGD?;;To increase learning rate over time;To reduce the batch size required;To compute second-order gradients;To avoid overfitting;To accelerate convergence and smooth oscillations;_alt5;Neural Nets;17
How does the Vision Transformer (ViT) reduce the number of input tokens compared to using one token per pixel?;;Uses only every third pixel;Uses CNN for dimensionality reduction;Applies PCA before feeding to transformer;Divides the image into fixed-size patches and embeds them;Applies dropout to pixels;_alt4;Vision Transformers;18
An input image of size $64 \times 64$ is passed through two convolutional layers: Conv1: kernel size 5, stride 2, padding 2. Conv2: kernel size 3, stride 2, padding 1. What is the spatial output size after both layers?;;$16 \times  16$;$32 \times 32$;$15 \times 15$;$17 \times 17$;$20 \times 20$;_alt1;CNNs;19
A 2D convolutional layer has: 64 input channels, 128 output channels, kernel size $3 \times 3$, no bias terms. How many trainable parameters does it have?;;147,456;110,592;81,920;73,728;196,608;_alt1;CNNs;20
Given gradient values $g = (2, 4)$, previous EMA$e = (1, 1)$, decay rate $\alpha = 0.9$ and learning rate $\eta = 0.01$, compute the RMSProp update step for the first component. Assume $\epsilon = 10^{-8}$, and focus only on the first component.;e_t = \alpha \cdot e_{t-1} + (1 - \alpha) \cdot g^2 \\ \theta = \theta - \eta \cdot \frac{g}{\sqrt{e_t + \epsilon}};-0.0061;-0.0193;-0.0120;-0.0058;-0.0035;_alt4;Neural Nets;21
An input of size $3 \times 3$ is passed through a transposed convolution with: kernel size 4, stride 2, padding 1. What is the spatial output size?;O = S \cdot (I - 1) + K - 2P;$8 \times 8$;$7 \times 7$;$6 \times 6$;$9  \times 9$;$10 \times 10$;_alt1;CNNs;22
What does covariate shift refer to?;;Training and test data have different label distributions;Training and test data have different input distributions ;The model overfits the test set;The labels are randomly shuffled in test data;Different models are used for training and testing;_alt2;Distribution Shifts;23
What is the challenge with label shift?;;The class distribution changes between training and test data;The training set contains incorrect labels;The inputs are corrupted with noise;The model is trained on the test labels;There are different numbers of samples in training and test;_alt1;Distribution Shifts;24
Under label shift, we assume that:;p(x|y)^{\text{train}} = p(x|y)^{\text{test}};$p(x∣y) \neq p(x|y)$;$p(y|x) \neq p(y|x)$;$p(x) = p(y)$;The conditional distribution $p(x | y)$ is the same in training and test;The training and test distributions are completely identical;_alt4;Distribution Shifts;25
What is the main limitation of standard feedforward networks on sequential data?;;They have too many parameters;They are slower than CNNs;They do not use backpropagation;They only work on images;They lack memory over time;_alt5;RNNs;26
What makes an RNN unique compared to feedforward networks?;;It shares weights across time and maintains a hidden state;It uses convolutions instead of matrix multiplications;It requires no training;It has no activation functions;It uses batch normalization for every time step;_alt1;RNNs;27
An RNN model typically minimizes the following loss function;\mathcal{L} = -\sum_{t=1}^T \log p(y_t | h_t);This is the loss function for CNNs;It only uses the final prediction;It minimizes loss for future steps;This is used for non-autoregressive models;This is a typical sequential log-loss;_alt5;RNNs;28
What is the goal of data augmentation?;;To generate new labels;To generate synthetic test sets;To improve generalization and model robustnes;To speed up training;To reduce batch size;_alt3;Data Augmentation;29
What kind of transformation is horizontal flipping?;;Geometric;Photometric;Statistical;Probabilistic;Affine;_alt1;Data Augmentation;30
What does a color jitter transformation do?;;Merges two images together;Applies back-translation;Adjusts spatial coordinates;Randomly alters hue, saturation, and brightness;Merges two class labels;_alt4;Data Augmentation;31
Which of the following properties shoud a data augmentation transformation T satisfy in supervised learning?;f(T(x)) = f(x);It should reduce the loss function directly.;It should modify labels to match the transformation.;It should preserve the semantic content of the input.;It should always decrease model accuracy.;It should only apply during inference.;_alt3;Data Augmentation;32
A model is trained on a dataset where 40% of samples are spam. In a future test set, the spam proportion drops to 10%, but the conditional distributions remain the same. What type of distribution shift is this?;;Covariate shift;Label shift;Concept drift;Prior shift;Domain adaptation;_alt2;Distribution Shifts;33
In an LSTM, why is the bias of the forget gate usually initialized to a high positive value?;C_t = f_t \odot C_{t-1} + i_t \odot \hat{C};To increase regularization.;To avoid vanishing gradients.;To encourage memory retention at the start of training.;To speed up convergence.;To maximize dropout impact.;_alt3;RNNs;34